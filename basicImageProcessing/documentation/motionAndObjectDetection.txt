Different pixel values = we can detect motion or change in the image.
Therefore, subtracting the image from the referent image means object & motion detection.
Also, defect detection...

To simplify processing we convert the image into greyscale (no RGB colors, only one value for colour channel).
Also, blurring the image (removing the edges) will reduce neccessary processing power. 
Not to be done if we do care about the edges.
After subrtracting, we get the third image, for which we can expect to have some extra features
other then the ones we are trying to detect, beacuse of lightning etc.
We convert that into B&W image. Then we dilate it, to group nearby pixels and extract
the largest cluster on the image. 
To apply this to video, we need to create a video feed and capture the first frame
as the reference frame and then compare every other frame to this reference frame.
Major drawback is that the reference frame is updated only once = changes in the 
lightning or shadows could be detected as objects. That means we need to update the 
reference frame regulary or average it.

Scripts:

https://github.com/bnbe-club/opencv-object-detection-diy-33