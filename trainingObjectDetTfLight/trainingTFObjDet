12.02.2021, training TF models using Object Detection API (TF OD API), which is an OS framework built on top of the TF,
which makes it easier to contruct, train and deploy object detection models.
TF2 OD API can be converted to TF Lite, but only SSD models are currently supported:
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tf2.md

To install it, use info on: 

https://anaconda.org/conda-forge/tf_object_detection
https://github.com/tensorflow/models/tree/master/research/object_detection

One can either use API to train and deploy a customized model or make statistical inference using pre-trained models from the model ZOO

Link below provides extensive documentation on how to use the API, configure the object detection pipeline and prepare inputs:
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md

Git repo had been cloned into the docker image, installed and tested. See documentationTFDockerSetup.txt for details

Faster R-CNN models are better suited to cases where high accuracy is desired and latency is of lower priority. 
Conversely, if processing time is the most important factor, SSD models are recommended. 

Training and Evaluation of the model:

https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_training_and_evaluation.md

First, a valid data set has to be created. TensorFlow Object Detection API reads data using the TFRecord file format.
Recycling the flow from the 
https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10
and
https://github.com/tensorflow/models/tree/079d67d9a0b3407e8d074a200780f3835413ef99

a generatTfRecord tool was first created, now part of the HELIOS Tools set.
Label the images and split them and their adjecent xml files in the images/train and images/test directories in the generateTfRecord tool folder.
Then run the xml_to_csv.py script.

Open generate_tfrecord.py and replace the label map that starts with the line 31 with the one used for this training data set, in this case, 
of only one object detection it looks like:

def class_text_to_int(row_label):
    if row_label == 'Object':
        return 1
    else:
        None

Additional case, deck of cards recognition:

def class_text_to_int(row_label):
    if row_label == 'nine':
        return 1
    elif row_label == 'ten':
        return 2
    elif row_label == 'jack':
        return 3
    elif row_label == 'queen':
        return 4
    elif row_label == 'king':
        return 5
    elif row_label == 'ace':
        return 6
    else:
        None


Create record files:
python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record
python generate_tfrecord.py --csv_input=images/test_labels.csv --image_dir=images/test --output_path=test.record

Last steps before training are creation of the label map and editing the config file.
Create labelmap.pbtxt file in the object_detection/data, or edit the existing one
to match the label map in generate_tfrecord.py

In the simplest case, it is just a file with this text:

item {
  id: 1
  name: 'Object'
}

With this, data set is prepared.


Model just needs to be configured using the config file. The config file calls the models from the installed models from the repo.

https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/configuring_jobs.md  This is how config is made

Following the instructions, one must now organize the folders. Inside the object_detection/models folder, create a directory with the 
name of the desired model, in this case HELIOS_1. 
Inside the directory, copy the folders that were downloaded from the pre-trained list of models, from the:
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md
These are pretty useful to initialize training on novel data sets.

Place the contents inside the HELIOS_1

Make the following changes to the config file of the desired model, usually labeled with pipeline.config


num_classes: 1 

train_config {
  batch_size: 6

 fine_tune_checkpoint: "/home/theia/_data/tfModels/models/research/object_detection/models/HELIOS_1/folder_name/ckpt-0"

eval_input_reader {
  label_map_path: "/home/theia/_data/tfModels/models/research/object_detection/data/labelmap.pbtxt"
  shuffle: false
  num_epochs: 1
  tf_record_input_reader {
    input_path: "/home/theia/_data/tfModels/models/research/object_detection/test.record"
  }


}
train_input_reader {
  label_map_path: "/home/theia/_data/tfModels/models/research/object_detection/data/labelmap.pbtxt"
  tf_record_input_reader {
    input_path: "/home/theia/_data/tfModels/models/research/object_detection/data/train.record"
  }







Inside the object_detection/data folder, copy the created train and test .record files. labelmap.pbtxt must also be there.

# From the tensorflow/models/research/ directory

PIPELINE_CONFIG_PATH=/home/theia/_data/tfModels/models/research/object_detection/models/HELIOS_1/pipeline.config

MODEL_DIR=/home/theia/_data/tfModels/models/research/object_detection/models/HELIOS_1

python object_detection/model_main_tf2.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --alsologtostderr




Last thing that was done to mitigate the GPU error:


Added the code below to the
/root/anaconda3/lib/python3.8/site-packages/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py
in the containter.

It looks like this is the model it calls....


# Included to mitigate issue: Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
# Vice: 13.02.2021

from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

#################################


Also, I've reduced the batch size to 1, but received again OOM error


To see all the processes that are using GPU:
sudo fuser -v /dev/nvidia*

To check GPU memommry usage:
nvidia-smi

To flush memmory
nvidia-smi --gpu-reset




Training completed! It was done using SSD Mobile Net ssd_mobilenet_v1_fpn_640x640_coco17_tpu-8 model whith limiting GPU usage:
This script is to be edited!

Relevant model python scripts in the docker image without anaconda:


# Included to mitigate issue: OOM NVDIA
# Vice: 13.02.2021

gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.6)
sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))

#################################


Other possible solutions (https://starriet.medium.com/tensorflow-2-0-wanna-limit-gpu-memory-10ad474e2528):

import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  try:
    for gpu in gpus:
      tf.config.experimental.set_memory_growth(gpu, True)
  except RuntimeError as e:
    print(e)




import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  try:
    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])
  except RuntimeError as e:
    print(e)


For TF 1.x

config = tf.ConfigProto()
config.gpu_options.allow_growth=True
sess = tf.Session(config=config)


# change the memory fraction as you want...
import tensorflow as tf
gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)
sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))


First training did not produced any results, no checkpoint values to export the inference graphs were created. The reasons were:
- faulty pipeline config path, which was corrected.
- checkpoints that are predetermined, fine tune checkoints, need to be in a folder inside the model that is not called "checkpoint" as 
tf will create a new file, also called checkpoint during it's training and it will be confused because it will have both the file to write new
values into and a directory under the same name. A folder called "hehehe" was created for the fine tuning checkpoints, and the config file was corrected as follows:

fine_tune_checkpoint: "/home/theia/_data/tfModels/models/research/object_detection/models/HELIOS_1/hehehe/ckpt-0"



Evaluate the training job

# From the tensorflow/models/research/ directory
PIPELINE_CONFIG_PATH=/home/theia/_data/tfModels/models/research/object_detection/models/HELIOS_1/pipeline.config

MODEL_DIR=/home/theia/_data/tfModels/models/research/object_detection/models/HELIOS_1

CHECKPOINT_DIR=/home/theia/_data/tfModels/models/research/object_detection/models/HELIOS_1

python object_detection/model_main_tf2.py \
    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
    --model_dir=${MODEL_DIR} \
    --checkpoint_dir=${CHECKPOINT_DIR} \
    --alsologtostderr


Converting to TFLite:

This step generates an intermediate SavedModel that can be used with the TFLite Converter via commandline or Python API.

# From the tensorflow/models/research/ directory
python object_detection/export_tflite_graph_tf2.py \
    --pipeline_config_path /home/theia/_data/tfModels/models/research/object_detection/models/HELIOS_1/pipeline.config \
    --trained_checkpoint_dir /home/theia/_data/tfModels/models/research/object_detection/models/HELIOS_1 \
    --output_directory /home/theia/_data/tfModels/models/research/object_detection/models/HELIOS_1/TFLite


Next, run the CLI converter. Beware that you give the path to the DIRECTORY of the saved model and the full path to
the output model, meaning also the name_of_the_output.tflite


tflite_convert \
  --saved_model_dir=/home/theia/_data/tfModels/models/research/object_detection/models/HELIOS_1/TFLite/saved_model \
  --output_file=/home/theia/_data/tfModels/models/research/object_detection/models/HELIOS_1/helios_1.tflite

  Finally, create a TFlite label map, which is just a text file with the names of the classes, listed using their IDs.
  In this case, only one word, "Object"



20.02.2021.
Model has been re-trained, this time with a batch size of 6 and 10 times more steps (25000). 
Resulting model performed pretty much the same as the one before.

Next, a loading, quantization optimization and exporting script using Python API has been created to be utiled for post processing
the model (to reduce it's size) while converting it to TFlite. The script is named modelLoadOptExpt.py and is placed in Tools folder in the repo.
To utilize the script properly, one must run it from the inside of the TF docker container. Script has to be placed in the same folder
where the intermediate SavedModel has been created using the CLI tool described in the step above. Now, CLI converter to TFlite does not need to be used anymore.
Size of the model reduced from 119 to 32 MB which is stil quite high, but considerably better.
Longer training incresed the accuracy.










