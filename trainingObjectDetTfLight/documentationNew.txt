Protocol buffers are Google's language-neutral, platform-neutral, 
extensible mechanism for serializing structured data like think XML, but smaller, faster, and simpler. 
TensorFlow (TF) is using them to store model training parameters, data labels, etc.

This entry is a re-written reconstruction of the 07.02.2021 entry on the next day, as the original file got corrupted.
As the file was on removable HDD, whole HDD was scanned and repaired using the Windows 10 tool. Corrupted file disappeared afterwards.
It will describe how to setup and use a TF docker container, on a Linux Ubuntu 18.04. machine.

First, install docker on the local host machine: https://docs.docker.com/get-docker/
To check the information about the docker account: docker info
Then, install NVDIA-Toolkit: https://github.com/NVIDIA/nvidia-docker, following further steps. Before starting,
one shall double check that the .450 or above NVDIA drivers are installed.
Guide: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker

Steps:

Setup the docker: 

curl https://get.docker.com | sh \
  && sudo systemctl --now enable docker

Setup stable repo and GPG key:

distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
   && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \
   && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

Install nvdia-docker 2:

sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker

Test the working setup:
sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi
This should give an table for an output, with mentioned GPU model, and driver and CUDA version.

Pull the latest version of TF docker with GPU support and Jupyter:
docker pull tensorflow/tensorflow:latest-gpu-jupyter

Try to run it: 

docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu

Type bash to start a bash session.

To check the currently open containers, in another terminal, do:
docker ps -a

To inspect the status of the container and output it's relevant information, do:
docker inspect <name of the container>

Stopping and removing containers:

docker stop <name of the container>
docker rm <name of the container>

Stoping and removing all the active containers:

docker stop $(docker container ls -aq)
docker rm $(docker container ls -aq)

To check is GPU available, look for it in VGA compatible section of the output from:
lspci


Now, to create a customized docker image. First, volumes in the docker container will be briefly addressed:
Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. 
While bind mounts are dependent on the directory structure and OS of the host machine, volumes are completely managed by Docker.
They have to be created outside the scope of the container. They can be more safely shared between multiple containers but also placed on cloud.

To create a volume:

docker volume create my-vol

To list volumes:

docker volume ls

Inspect a volume:

docker volume inspect my-vol

Remove a volume:

docker volume rm my-vol

Default location where the volumes are stored is: /var/lib/docker/volumes
For the purpose of this project, a volume called theia has been created. Inside the volume, docker automatically created _data folder.
That folder and explicitly that folder must be used to place data that one wants to use inside the docker container!

To start the container with the volume mounted, and to change the working directory of the container to that mounted point, do:

docker run --gpus all -it --rm -v /var/lib/docker/volumes/theia:/home/theia -w /home/theia tensorflow/tensorflow:latest-gpu

Next, several dependencies will be installed inside the docker TF container and those images will be commited and saved locally, so they 
can be used as a working framework.
N.B. One must start the docker container with root permissions! Furthemore, mounting the volume needs to be done on every startup of the contanier
as it is no "burned" in the image, to corresponds to the volumes independent structure.

First, Anaconda package was installed (https://docs.anaconda.com/anaconda/install/linux/) by downloading the recquired file, moving it to the volume
and then first verifying the installation with:

sha256sum /path/filename

Subsequently, installation was run with:

bash ~/Downloads/Anaconda3-2020.11-Linux-x86_64.sh

From the Anaconda pakckage, several other libraries were installed:

conda install -c anaconda pyqt
conda install -c pytorch pytorch

This one lasted...
conda install -c conda-forge opencv
conda install -c conda-forge keras

To commit the changes, find the image ID using: docker ps -a
Then do:

sudo docker commit [CONTAINER_ID] [new_image_name]

To save the image locally:

docker save -o <image_file_name>.tar <image_name>


To load image: 

docker load -i <image_file_name>.tar

Starting the image is done same as before, but using the image name provided in the ouput of the previous command.
F.ex.:
docker run --gpus all -it --rm -v /var/lib/docker/volumes/theia:/home/theia -w /home/theia helios_0.1.0:latest

Use python script.py to run your code.